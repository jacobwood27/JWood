<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/libs/katex/katex.min.css"> <link rel=stylesheet  href="/libs/highlight/github.min.css"> <link rel=stylesheet  href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"> <link rel=stylesheet  href="/css/franklin.css"> <link rel=stylesheet  href="/css/basic.css"> <script data-goatcounter="https://jacobwxyz.goatcounter.com/count" async src="//gc.zgo.at/count.js"></script> <script async src="https://storage.googleapis.com/al-web-sdk/al.min.js"></script> <script> window.ALSDK = window.ALSDK || {detectWallet:async()=>{return new Promise(async(resolve)=>{document.addEventListener('absolutelabs.walletDetected',(e)=>{resolve(e.detail)})})}}; ALSDK.siteId = '23ee1cfc'; </script> <link rel=icon  href="/assets/favicon.png"> <title>Shuffle</title> <header> <div class=blog-name ><a href="/">Jacob Wood</a></div> <nav> <ul> <li><a href="/projects/">Projects</a> <li><a href="/books/">Books</a> <li><a href="/cv/">CV</a> <li><a href="/now/">Now</a> </ul> <img src="/assets/hamburger.svg" id=menu-icon > </nav> </header> <div class=franklin-content ><div class=note ><div class=title >Objective</div> <div class=content >Identify a small experiment and take it through the process of gathering data, modeling, and analyzing.</div></div> <p>Folk wisdom claims 7 shuffles is sufficient to thoroughly mix up a deck of cards. This claim originates from a <a href="https://escholarship.org/content/qt0k4654kx/qt0k4654kx.pdf?t&#61;p3z6d7">paper</a> published in 1986 by David Aldous and Persi Diaconis and summarized <a href="https://www.nytimes.com/1990/01/09/science/in-shuffling-cards-7-is-winning-number.html">in the New York Times</a> in 1990. </p> <p>Does my mediocre shuffling reflect this common wisdom? This post is an attempt to investigate that question.</p> <hr /> <p><strong>Table of Contents</strong> <div class=franklin-toc ><ol><li><a href="#collecting_data">Collecting Data</a><ol><li><a href="#shuffling_-_head_on">Shuffling - Head On </a><li><a href="#shuffling_-_angled_down">Shuffling - Angled Down</a><li><a href="#post_shuffle_conveyor_belt">Post Shuffle Conveyor Belt</a><li><a href="#post_shuffle_riffle">Post Shuffle Riffle</a><ol><li><a href="#transfer_learning_post_processing">Transfer Learning Post Processing</a></ol></ol><li><a href="#building_a_shuffling_model">Building a Shuffling Model</a><ol><li><a href="#data_exploration">Data Exploration</a><li><a href="#building_a_model">Building a Model</a><ol><li><a href="#deck_split">Deck Split</a></ol><li><a href="#dropping_cards">Dropping Cards</a></ol><li><a href="#investigating_performance">Investigating Performance</a><li><a href="#ingredients">Ingredients</a></ol></div></p> <hr /> <h1 id=collecting_data ><a href="#collecting_data" class=header-anchor >Collecting Data</a></h1> <p>The first step to assessing my shuffling performance is to investigate a few real world shuffles. To start I recorded 5 the results of 5 shuffles by hand to get a sense of the variation in the distribution. Each run involved shuffling the deck with a riffle shuffle but not aligning the left and right hand piles:</p> <div class=im-100 ><img src="/posts/005_shuffle/card_shuffle1.png" alt=""></div> <p>The resulting order was then recorded as a string of 1s &#40;card from left hand&#41; and 2s &#40;card from right hand&#41;. The 5 initial shuffles were:</p> <pre><code class="plaintext hljs">1112212122121212121212121212121212121212121212111221
1222212212212121221212121212121212222112112211221122
2221121212211212112121221211221212121122222112212122
2222122121221121212121122112211222121222211222112211
2221221221122221212112221122121212212211122122121221</code></pre> <p>Eyeballing here shows quite a bit of variation from shuffle to shuffle. I set out to record the results of 100 independent shuffles to start to quantify the distribution.</p> <h2 id=shuffling_-_head_on ><a href="#shuffling_-_head_on" class=header-anchor >Shuffling - Head On </a></h2> <p>The most straightforward approach seemed to be recording the cards falling during the shuffling sequence in slow motion &#40;phone camera can capture at 240fps&#41;. The video was recorded in a semi-reproducible environment to ease processing:</p> <div class=im-60 ><img src="/posts/005_shuffle/front_view.png" alt=""></div> <p>A sample of the resulting video looks like:</p> <div class=vid-100 ><video controls mute autoplay loop> <source src="/posts/005_shuffle/front_shuffle.webm" type="video/webm"> </video></div> <p>Even at 240fps some of the faster cards aren&#39;t captured in transit, and the ones that are captured are blurry and tough to detect with any sort of definable edge or feature. We can address this by instead capturing the face of the shuffle pile as it changes &#40;the time constant between dropping new cards is much larger than the one associated with the drop itself&#41;.</p> <h2 id=shuffling_-_angled_down ><a href="#shuffling_-_angled_down" class=header-anchor >Shuffling - Angled Down</a></h2> <p>The new and improved data collection environment involved even more cardboard:</p> <div class=im-60 ><img src="/posts/005_shuffle/top_view.png" alt=""></div> <p>Initially, the video collected during a shuffle looks promising. It is easy to identify transitions between shuffled cards and which hand is dropping the card:</p> <div class=vid-100 ><video controls mute autoplay loop> <source src="/posts/005_shuffle/top_shuffle.webm" type="video/webm"> </video></div> <p>However, when we look a little closer we don&#39;t notice any funny business:</p> <div class=vid-100 ><video controls mute autoplay loop> <source src="/posts/005_shuffle/top_slow2.webm" type="video/webm"> </video></div> <p>Which is a problem because there was indeed some funny business. The 3 of clubs was tucked between the 9 and 4 of spades. </p> <div class=im-60 ><img src="/posts/005_shuffle/missing3c.png" alt=""></div> <p>You can make out the edge of the card in the video, but we never see the face because the shuffle isn&#39;t perfect and I release both the 3 of clubs and the 4 of spades at the same time. </p> <p>This doesn&#39;t happen in every shuffle, but it does happen occasionally. We might argue that we can just throw out shuffles where we don&#39;t process 52 different cards to avoid the missed card measurement error. However, that would introduce a systematic bias into the measurement - we would be removing all the worst shuffles from the dataset and our resulting impression of our shuffling performance would be better than it should be.</p> <h2 id=post_shuffle_conveyor_belt ><a href="#post_shuffle_conveyor_belt" class=header-anchor >Post Shuffle Conveyor Belt</a></h2> <p>We had a lot of cardboard to spare and I had to try getting the power tools involved somehow. The gravity-fed rubber band and drill feeder worked, but was more trouble than it was worth.</p> <div class=vid-100 ><video controls mute autoplay loop> <source src="/posts/005_shuffle/bottom_belt.webm" type="video/webm"> </video></div> <h2 id=post_shuffle_riffle ><a href="#post_shuffle_riffle" class=header-anchor >Post Shuffle Riffle</a></h2> <p>One way to address the systematic bias mentioned above is to decouple the event of the shuffle from the recording of it. That way, if we were to make an error that invalidates the recording, we have no reason to expect the error would alter the perceived distribution. One way to do this is to record the order of the cards after the shuffle. If we record the order before and after each shuffle we can back out the 1s and 2s that make up our riffle model.</p> <p>If we take the cards and riffle them in front of the camera we can then look for cards in each frame and record the order. </p> <div class=vid-100 ><video controls mute autoplay loop> <source src="/posts/005_shuffle/post_riffle_slow.webm" type="video/webm"> </video></div> <p>This strategy is also prone to missing a card, but we can address the problem this time. We can record two different riffles and compare the resulting orders, using the knowledge from the second recording to fill in gaps or resolve discrepancies. If we are unable to order the cards with sufficient confidence we can drop that shuffle from the dataset without introducing bias.</p> <h3 id=transfer_learning_post_processing ><a href="#transfer_learning_post_processing" class=header-anchor >Transfer Learning Post Processing</a></h3> <p>Of course, we won&#39;t be processing the video by hand. I was hoping to use this project to do some in-the-wild machine learning so we will leverage that here. There are a <a href="https://github.com/search?q&#61;playing&#43;card&#43;detection">few projects</a> on Github doing playing card detection but they are generally looking for whole cards on specific backdrops. We will train a new model and make use of the controlled environment it is being deployed in.</p> <h4 id=data_labelling ><a href="#data_labelling" class=header-anchor >Data Labelling</a></h4> <p>We&#39;ll start off by labelling a bit of the data we collected to train on. We&#39;ll use Python and OpenCV to flip through the video files and write frames:</p> <pre><code class="Python hljs"><span class=hljs-keyword >import</span> cv2</code></pre>
<p>Most of the models we will be considering want small and square images as input. Fortunately, we riffled in a very consistent fashion so we can crop all the video frames in the same small square. We&#39;ll use 224x224 as our image size for now.</p>
<div class=im-60 ><img src="/posts/005_shuffle/sample_full_frame_rect.jpg" alt=""></div>
<pre><code class="Python hljs"><span class=hljs-keyword >def</span> <span class="hljs-title function_">prep_frame</span>(<span class=hljs-params >f</span>):
    <span class=hljs-keyword >return</span> cv2.resize(f[<span class=hljs-number >0</span>:<span class=hljs-number >800</span>, <span class=hljs-number >700</span>:<span class=hljs-number >1500</span>], (<span class=hljs-number >224</span>,<span class=hljs-number >224</span>))</code></pre>
<p>We can then run through the video frame by frame and label each image. To make things easier we will consider two different classification problems for each frame independently - suit and rank. </p>
<p>To start with the suits we need to:</p>
<ol>
<li><p>Read a new frame from a video</p>

<li><p>Crop the frame as dictated above</p>

<li><p>Show the frame and wait for a keypress</p>

<li><p>Upon keypress:</p>

</ol>
<ul>
<li><p>If &quot;c&quot;, &quot;s&quot;, &quot;h&quot;, &quot;d&quot;, or &quot;0&quot; &#40;for clubs, spades, hearts, diamonds, none&#41; save to that directory with unique ID generated from frame counter</p>

<li><p>If &quot;q&quot; then quit the program</p>

<li><p>If other key then skip the frame and go to the next</p>

</ul>
<ol start=5 >
<li><p>Repeat until the end of the video is reached </p>

</ol>
<pre><code class="Python hljs">cap = cv2.VideoCapture(<span class=hljs-string >&quot;INPUT.MOV&quot;</span>)

i = <span class=hljs-number >0</span>

<span class=hljs-keyword >while</span> <span class=hljs-literal >True</span>:
    
    i+=<span class=hljs-number >1</span>

    ret, f = cap.read()
    <span class=hljs-keyword >if</span> <span class=hljs-keyword >not</span> ret:
        <span class=hljs-built_in >print</span>(<span class=hljs-string >&quot;End of video&quot;</span>)
        <span class=hljs-keyword >break</span>

    f = prep_frame(f)
    
    cv2.imshow(<span class=hljs-string >&#x27;frame&#x27;</span>, f)
    key = cv2.waitKey(<span class=hljs-number >0</span>)
    
    <span class=hljs-keyword >if</span> <span class=hljs-built_in >chr</span>(key) <span class=hljs-keyword >in</span> <span class=hljs-string >&#x27;cshd0&#x27;</span>:
        cv2.imwrite(<span class=hljs-string >&quot;suit_data/&quot;</span> + <span class=hljs-built_in >chr</span>(key) + <span class=hljs-string >&quot;/&quot;</span> + <span class=hljs-built_in >str</span>(i) + <span class=hljs-string >&quot;.jpg&quot;</span>, f)
    <span class=hljs-keyword >elif</span> <span class=hljs-built_in >chr</span>(key) <span class=hljs-keyword >in</span> <span class=hljs-string >&#x27;q&#x27;</span>:
        <span class=hljs-keyword >break</span>

cap.release()
cv2.destroyAllWindows()</code></pre>
<p>After running through all the video frames we should have generated a bit of training data for each suit.</p>
<div class=im-60 ><img src="/posts/005_shuffle/post_suit_class.png" alt=""></div>
<p>And we can verify the classifications look decent:</p>
<pre><code class="Python hljs"><span class=hljs-keyword >import</span> matplotlib.pyplot <span class=hljs-keyword >as</span> plt
<span class=hljs-keyword >from</span> mpl_toolkits.axes_grid1 <span class=hljs-keyword >import</span> ImageGrid
<span class=hljs-keyword >import</span> numpy <span class=hljs-keyword >as</span> np

fig = plt.figure(figsize=(<span class=hljs-number >10.</span>, <span class=hljs-number >6.</span>))
grid = ImageGrid(fig, <span class=hljs-number >111</span>, nrows_ncols=(<span class=hljs-number >3</span>, <span class=hljs-number >5</span>), axes_pad=<span class=hljs-number >0.1</span>, share_all=<span class=hljs-literal >True</span>)

grid[<span class=hljs-number >0</span>].get_yaxis().set_ticks([])
grid[<span class=hljs-number >0</span>].get_xaxis().set_ticks([])

grid[<span class=hljs-number >0</span>].set_title(<span class=hljs-string >&quot;C&quot;</span>)
grid[<span class=hljs-number >1</span>].set_title(<span class=hljs-string >&quot;D&quot;</span>)
grid[<span class=hljs-number >2</span>].set_title(<span class=hljs-string >&quot;H&quot;</span>)
grid[<span class=hljs-number >3</span>].set_title(<span class=hljs-string >&quot;S&quot;</span>)
grid[<span class=hljs-number >4</span>].set_title(<span class=hljs-string >&quot;0&quot;</span>)

ims = []
<span class=hljs-keyword >for</span> _ <span class=hljs-keyword >in</span> <span class=hljs-built_in >range</span>(<span class=hljs-number >3</span>):
    <span class=hljs-keyword >for</span> s <span class=hljs-keyword >in</span> <span class=hljs-string >&quot;cdhs0&quot;</span>:
        im_file = random.choice(os.listdir(<span class=hljs-string >&quot;suit_data/&quot;</span>+s))
        im = cv2.imread(<span class=hljs-string >&quot;suit_data/&quot;</span>+s+<span class=hljs-string >&quot;/&quot;</span>+im_file)
        ims.append(im[...,::-<span class=hljs-number >1</span>])

<span class=hljs-keyword >for</span> ax, im <span class=hljs-keyword >in</span> <span class=hljs-built_in >zip</span>(grid, ims):
    ax.imshow(im)

plt.show()</code></pre>
<div class=im-100 ><img src="/posts/005_shuffle/suit_verify.png" alt=""></div>
<p>Rinse and repeat for the ranks of each card.</p>
<h4 id=model_training ><a href="#model_training" class=header-anchor >Model Training</a></h4>
<p>Now that we have a subset of our data labelled we can train an algorithm to label the rest of the videos we will take. </p>
<p>There are plenty of models out there that do a great job of parsing images and learning their requisite features. We would like to take a model that is good at understanding images and have it learn to classify our specific images. This is the definition of <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a>. </p>
<p>We&#39;ll follow along with the TensorFlow example detailed <a href="https://www.tensorflow.org/hub/tutorials/tf2_image_retraining">here</a> for the most part.</p>
<p>First bring in a few packages we will need:</p>
<pre><code class="Python hljs"><span class=hljs-keyword >import</span> os

<span class=hljs-keyword >import</span> matplotlib.pylab <span class=hljs-keyword >as</span> plt
<span class=hljs-keyword >import</span> numpy <span class=hljs-keyword >as</span> np

<span class=hljs-keyword >import</span> tensorflow <span class=hljs-keyword >as</span> tf
<span class=hljs-keyword >import</span> tensorflow_hub <span class=hljs-keyword >as</span> hub</code></pre>
<p>The we can browse through the <a href="https://tfhub.dev/s?module-type&#61;image-classification">available models on TensorFlow Hub</a> for a suitable classification model. Our problem is pretty easy, and we will have no trouble using a small <a href="https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/5">MobileNet model</a>. This model takes in 224x224 images &#40;which is what we saved our training data at, so no need to resize&#41;.</p>
<pre><code class="Python hljs">model_name = <span class=hljs-string >&quot;mobilenet_v2_100_224&quot;</span>
model_handle = <span class=hljs-string >&quot;https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/5&quot;</span>
image_size = (<span class=hljs-number >224</span>,<span class=hljs-number >224</span>)
batch_size = <span class=hljs-number >16</span>
data_dir = <span class=hljs-string >&quot;suit_data&quot;</span></code></pre>
<p>We&#39;ll use <a href="https://keras.io/">Keras</a> as our framework. Keras provide a convenient API to split up our dataset:</p>
<pre><code class="Python hljs"><span class=hljs-keyword >def</span> <span class="hljs-title function_">build_dataset</span>(<span class=hljs-params >subset</span>):
      <span class=hljs-keyword >return</span> tf.keras.preprocessing.image_dataset_from_directory(
        data_dir,
        validation_split=<span class=hljs-number >.20</span>,
        subset=subset,
        label_mode=<span class=hljs-string >&quot;categorical&quot;</span>,
        seed=<span class=hljs-number >123</span>,
        image_size=image_size,
        batch_size=<span class=hljs-number >1</span>)</code></pre>
<p>Which we can use to generate a training and a validation dataset:</p>
<pre><code class="Python hljs">train_ds    = build_dataset(<span class=hljs-string >&quot;training&quot;</span>)
val_ds      = build_dataset(<span class=hljs-string >&quot;validation&quot;</span>)</code></pre>
<p>And record a few pieces of information before we change the datasets:</p>
<pre><code class="Python hljs">train_size  = train_ds.cardinality().numpy()
val_size    =   val_ds.cardinality().numpy()
class_names = <span class=hljs-built_in >tuple</span>(train_ds.class_names)</code></pre>
<p>We will want to pre-process our image inputs and introduce some artificial warping to flesh out the data set. </p>
<p>First, the MobileNet model expects inputs from 0 to 1 and our current images contain pixel values from 0 to 255. We can fix that with a normalization layer that scales the data by 1/255. We&#39;ll want to do this to both the training and the validation data.</p>
<p>Next, we can move the images around a bit to resemble changes we might see in new data. These changes could consist of:</p>
<ul>
<li><p>rotation</p>

<li><p>vertical translation</p>

<li><p>horizontal translation </p>

<li><p>zoom</p>

<li><p>contrast </p>

</ul>
<p>We will only want to apply these mutations to the training data.</p>
<pre><code class="Python hljs">normalization_layer = tf.keras.layers.Rescaling(<span class=hljs-number >1.</span> / <span class=hljs-number >255</span>)

preprocessing_train = tf.keras.Sequential([
    normalization_layer,
    tf.keras.layers.RandomRotation(<span class=hljs-number >0.1</span>),
    tf.keras.layers.RandomTranslation(<span class=hljs-number >0</span>, <span class=hljs-number >0.2</span>),
    tf.keras.layers.RandomTranslation(<span class=hljs-number >0.2</span>, <span class=hljs-number >0</span>),
    tf.keras.layers.RandomZoom(<span class=hljs-number >0.2</span>, <span class=hljs-number >0.2</span>),
    tf.keras.layers.RandomContrast(<span class=hljs-number >0.1</span>),
])

preprocessing_val = tf.keras.Sequential([
    normalization_layer
])</code></pre>
<p>Then we can take our data, stick it into batches, and apply the preprocessing described above to get it all ready to go. Note - we need to add a repeat&#40;&#41; call to our training data to ensure we can make enough data during the training runs &#40;this seems weird to me?&#41;. </p>
<pre><code class="Python hljs">train_ds = train_ds.unbatch().batch(batch_size)
train_ds = train_ds.repeat()
train_ds = train_ds.<span class=hljs-built_in >map</span>(<span class=hljs-keyword >lambda</span> images, labels:(preprocessing_train(images), labels))

val_ds = val_ds.unbatch().batch(batch_size)
val_ds = val_ds.<span class=hljs-built_in >map</span>(<span class=hljs-keyword >lambda</span> images, labels:(preprocessing_val(images), labels))</code></pre>
<p>With the data ready to go we can prepare the model we are going to train. We need to add a few things to the main MobileNet model specified earlier to link everything together.</p>
<p>First, we need to add an input layer that is compatible with our RGB image size &#40;224x224x3&#41;. </p>
<p>That can feed into the MobileNet model which gets downloaded from TensorFlow Hub. </p>
<p>We probably also want to include some dropout to prevent overtraining. 20&#37; is a standard value to start with.</p>
<p>Finally, we need a Dense layer that will act as the classifier for the 5 different classes we have: none, clubs, diamonds, hearts, and spades. We will include some <a href="https://developers.google.com/machine-learning/glossary/#L2_regularization">L2 regularization</a> in this Dense layer to keep the kernel weights in check.</p>
<pre><code class="Python hljs">model = tf.keras.Sequential([
    tf.keras.layers.InputLayer(input_shape=image_size + (<span class=hljs-number >3</span>,)),
    hub.KerasLayer(model_handle, trainable=<span class=hljs-literal >True</span>),
    tf.keras.layers.Dropout(rate=<span class=hljs-number >0.2</span>),
    tf.keras.layers.Dense(<span class=hljs-built_in >len</span>(class_names),
                          kernel_regularizer=tf.keras.regularizers.l2(<span class=hljs-number >0.0001</span>))
])</code></pre>
<p>Next we need to define how we want to train the model. This is done with model.compile&#40;&#41; and some definitions. </p>
<ul>
<li><p>Optimizer: <a href="https://keras.io/api/optimizers/sgd/">Stochastic Gradient Descent</a> with a small learning rate should work just fine, but feel free to poke around</p>

<li><p>Loss Function: <a href="https://keras.io/api/losses/probabilistic_losses/#categoricalcrossentropy-class">Categorical Crossentropy</a> is recommended when there are two or more labels that are one-hot encoded</p>
<ul>
<li><p>from_logits &#61; True <a href="https://datascience.stackexchange.com/questions/73093/what-does-from-logits-true-do-in-sparsecategoricalcrossentropy-loss-function">must be used</a> when the outputs are not normalized &#40;as is the case when we have not soft-maxed the outputs&#41;</p>

<li><p>label_smoothing takes our one-hot encoded outputs and smooths out the confidence a bit. Instead of encoding a club as &#91;0,1,0,0,0&#93; we encode it as, say, &#91;0.01,0.96,0.01,0.01,0.01&#93;. This can help regularization a bit.</p>

</ul>

<li><p>Metrics: We will just monitor accuracy here</p>

</ul>
<pre><code class="Python hljs">model.<span class=hljs-built_in >compile</span>(
    optimizer=tf.keras.optimizers.SGD(learning_rate=<span class=hljs-number >0.005</span>), 
    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=<span class=hljs-literal >True</span>, label_smoothing=<span class=hljs-number >0.1</span>),
    metrics=[<span class=hljs-string >&#x27;accuracy&#x27;</span>])</code></pre>
<p>Now we are ready to hit go on the training. We will train the model for 10 standard epochs and see how we&#39;re doing. This might take a few minutes.</p>
<pre><code class="Python hljs">steps_per_epoch = train_size // batch_size
validation_steps = val_size // batch_size
hist = model.fit(
    train_ds,
    epochs=<span class=hljs-number >10</span>, 
    steps_per_epoch=steps_per_epoch,
    validation_data=val_ds,
    validation_steps=validation_steps).history</code></pre>
<pre><code class="Plaintext hljs">Epoch 1/10
45/45 [==============================] - 44s 908ms/step - loss: 1.1458 - accuracy: 0.5792 - val_loss: 0.9850 - val_accuracy: 0.6420
Epoch 2/10
45/45 [==============================] - 39s 884ms/step - loss: 0.7227 - accuracy: 0.8610 - val_loss: 0.6280 - val_accuracy: 0.9205
Epoch 3/10
45/45 [==============================] - 40s 879ms/step - loss: 0.5971 - accuracy: 0.9368 - val_loss: 0.4976 - val_accuracy: 0.9886
Epoch 4/10
45/45 [==============================] - 40s 878ms/step - loss: 0.5762 - accuracy: 0.9579 - val_loss: 0.5069 - val_accuracy: 0.9943
Epoch 5/10
45/45 [==============================] - 39s 875ms/step - loss: 0.5431 - accuracy: 0.9691 - val_loss: 0.4847 - val_accuracy: 0.9943
Epoch 6/10
45/45 [==============================] - 39s 877ms/step - loss: 0.5381 - accuracy: 0.9705 - val_loss: 0.4756 - val_accuracy: 1.0000
Epoch 7/10
45/45 [==============================] - 40s 878ms/step - loss: 0.5272 - accuracy: 0.9803 - val_loss: 0.4778 - val_accuracy: 0.9886
Epoch 8/10
45/45 [==============================] - 40s 879ms/step - loss: 0.5193 - accuracy: 0.9803 - val_loss: 0.4799 - val_accuracy: 0.9886
Epoch 9/10
45/45 [==============================] - 39s 878ms/step - loss: 0.5227 - accuracy: 0.9789 - val_loss: 0.4841 - val_accuracy: 0.9830
Epoch 10/10
45/45 [==============================] - 39s 873ms/step - loss: 0.5221 - accuracy: 0.9733 - val_loss: 0.4808 - val_accuracy: 0.9943</code></pre>
<p>Tough to beat that. We probably could have gotten away with only 5 epochs, but oh well.</p>
<div class=im-100 ><img src="/posts/005_shuffle/train_loss.png" alt=""></div>
<p>Finally, we can save the trained model:</p>
<pre><code class="Python hljs">model.save(<span class=hljs-string >&quot;suit_predictor&quot;</span>)</code></pre>
<h4 id=deploying_the_model ><a href="#deploying_the_model" class=header-anchor >Deploying the Model </a></h4>
<p>Now we need to use our models to identify all the cards in a riffle as it goes by and back out the shuffled string of 1s and 2s.</p>
<p>We&#39;ll start by loading in our models and the classes the predictions represent:</p>
<pre><code class="Python hljs">ranks = (<span class=hljs-string >&#x27;0&#x27;</span>, <span class=hljs-string >&#x27;2&#x27;</span>, <span class=hljs-string >&#x27;3&#x27;</span>, <span class=hljs-string >&#x27;4&#x27;</span>, <span class=hljs-string >&#x27;5&#x27;</span>, <span class=hljs-string >&#x27;6&#x27;</span>, <span class=hljs-string >&#x27;7&#x27;</span>, <span class=hljs-string >&#x27;8&#x27;</span>, <span class=hljs-string >&#x27;9&#x27;</span>, <span class=hljs-string >&#x27;a&#x27;</span>, <span class=hljs-string >&#x27;j&#x27;</span>, <span class=hljs-string >&#x27;k&#x27;</span>, <span class=hljs-string >&#x27;q&#x27;</span>, <span class=hljs-string >&#x27;z&#x27;</span>)
rank_model = tf.keras.models.load_model(<span class=hljs-string >&quot;rank_predictor&quot;</span>)

suits = (<span class=hljs-string >&#x27;0&#x27;</span>, <span class=hljs-string >&#x27;c&#x27;</span>, <span class=hljs-string >&#x27;d&#x27;</span>, <span class=hljs-string >&#x27;h&#x27;</span>, <span class=hljs-string >&#x27;s&#x27;</span>)
suit_model = tf.keras.models.load_model(<span class=hljs-string >&quot;suit_predictor&quot;</span>)

image_size = (<span class=hljs-number >224</span>, <span class=hljs-number >224</span>)</code></pre>
<p>We also need to make sure we do the same preprocessing on new images that we did on previous ones. That means cropping, scaling, and normalizing:</p>
<pre><code class="Python hljs"><span class=hljs-keyword >def</span> <span class="hljs-title function_">prep_frame</span>(<span class=hljs-params >f</span>):
    <span class=hljs-keyword >return</span> cv2.resize(f[<span class=hljs-number >0</span>:<span class=hljs-number >800</span>, <span class=hljs-number >700</span>:<span class=hljs-number >1500</span>], (<span class=hljs-number >224</span>,<span class=hljs-number >224</span>))

normalization_layer = tf.keras.layers.Rescaling(<span class=hljs-number >1.</span> / <span class=hljs-number >255</span>)
preprocessing_model = tf.keras.Sequential([normalization_layer])
<span class=hljs-keyword >def</span> <span class="hljs-title function_">prep_input</span>(<span class=hljs-params >inp</span>):
    f = prep_frame(inp)
    arr = np.array([f[...,::-<span class=hljs-number >1</span>].astype(np.float32)])
    <span class=hljs-keyword >return</span> preprocessing_model(arr)</code></pre>
<p>It is also going to be helpful to label the images directly as we monitor some results. We will borrow the draw_text function from <a href="https://stackoverflow.com/questions/60674501/how-to-make-black-background-in-cv2-puttext-with-python-opencv">this</a> stackoverflow answer:</p>
<pre><code class="Python hljs"><span class=hljs-keyword >def</span> <span class="hljs-title function_">draw_text</span>(<span class=hljs-params >img, text, font=cv2.FONT_HERSHEY_PLAIN, pos=(<span class=hljs-params ><span class=hljs-number >0</span>, <span class=hljs-number >0</span></span>), font_scale=<span class=hljs-number >3</span>, font_thickness=<span class=hljs-number >2</span>, text_color=(<span class=hljs-params ><span class=hljs-number >0</span>, <span class=hljs-number >0</span>, <span class=hljs-number >0</span></span>), text_color_bg=(<span class=hljs-params ><span class=hljs-number >255</span>, <span class=hljs-number >255</span>, <span class=hljs-number >255</span></span>)</span>):
    x, y = pos
    text_size, _ = cv2.getTextSize(text, font, font_scale, font_thickness)
    text_w, text_h = text_size
    cv2.rectangle(img, pos, (x + text_w, y + text_h), text_color_bg, -<span class=hljs-number >1</span>)
    cv2.putText(img, text, (x, y + text_h + font_scale - <span class=hljs-number >1</span>), font, font_scale, text_color, font_thickness)</code></pre>
<p>Now we can see how we did by making predictions on new video frames. To prevent spurious classifications we will only record a card if we ID it two frames in a row.</p>
<pre><code class="Python hljs">cap = cv2.VideoCapture(<span class=hljs-string >&quot;INPUT2.MOV&quot;</span>)

last_card = <span class=hljs-string >&quot;&quot;</span>
card_order = []
viz = <span class=hljs-literal >True</span>

<span class=hljs-keyword >while</span> <span class=hljs-literal >True</span>:
    ret, frame = cap.read()
    <span class=hljs-keyword >if</span> <span class=hljs-keyword >not</span> ret:
        <span class=hljs-built_in >print</span>(<span class=hljs-string >&quot;No frame&quot;</span>)
        <span class=hljs-keyword >break</span>
            
    inp = prep_input(frame)
    
    preds = suit_model.predict(inp)
    suit_pred = suits[np.argmax(preds)]

    preds = rank_model.predict(inp)
    rank_pred = ranks[np.argmax(preds)]

    <span class=hljs-keyword >if</span> rank_pred!=<span class=hljs-string >&quot;z&quot;</span> <span class=hljs-keyword >and</span> suit_pred!=<span class=hljs-string >&quot;0&quot;</span>:
        card = rank_pred + suit_pred
        <span class=hljs-keyword >if</span> card == last_card <span class=hljs-keyword >and</span> (<span class=hljs-built_in >len</span>(card_order)==<span class=hljs-number >0</span> <span class=hljs-keyword >or</span> card_order[-<span class=hljs-number >1</span>] != card):
            card_order.append(card)
        <span class=hljs-keyword >if</span> card == last_card <span class=hljs-keyword >and</span> viz:
            draw_text(frame, rank_pred + suit_pred, pos=(<span class=hljs-number >800</span>, <span class=hljs-number >500</span>))
        last_card = card
        
    <span class=hljs-keyword >if</span> viz:
        cv2.imshow(<span class=hljs-string >&#x27;frame&#x27;</span>, frame)
        <span class=hljs-keyword >if</span> cv2.waitKey(<span class=hljs-number >3</span>) == <span class=hljs-built_in >ord</span>(<span class=hljs-string >&#x27;q&#x27;</span>):
            <span class=hljs-keyword >break</span>

cv2.destroyAllWindows()</code></pre>
<div class=vid-100 ><video controls mute autoplay loop>
  <source src="/posts/005_shuffle/class_detect.webm" type="video/webm">
</video></div>
<p>If the video we processed above has two independent riffles in it we should end up with a <code>card_order</code> vector that is, ideally, 104 elements long with 52 unique elements. That probably won&#39;t be the case. Instead, we probably get something that looks like:</p>
<pre><code class="Plaintext hljs">0d
8c
jc
0h
jh
9s
9d
kh
.
.
.
0s
9h
9d
as
0d
8c
jc
0h
jh
8s
9s
9d
kh
.
.
.
0s
9h</code></pre>
<p>Our two most robust measurements should be the last card of the first riffle and the last card of the second riffle. We can use these to split the vector in half and start lining things up &#40;we could also detect the break between riffle 1 and riffle 2 using the timestamps in the video&#41;. In the example above the last card would be the <code>9h</code>.</p>
<pre><code class="Plaintext hljs">After 9h split and align:

    9d
    as
    0d
0d  8c
8c  jc
jc  0h
0h  jh
jh  8s
9s  9s
9d  9d
kh  kh
.   .
.   .
.   .
0s  0s
9h  9h</code></pre>
<p>Now we can crawl through the two vectors and look for similar neighbors. On the first pass we will just note what is missing and mark it with an empty character:</p>
<pre><code class="Python hljs">num_el = <span class=hljs-built_in >len</span>(<span class=hljs-built_in >set</span>(card_order))

last_card = card_order[-<span class=hljs-number >1</span>]

r1 = card_order[:card_order.index(last_card)+<span class=hljs-number >1</span>]
r2 = card_order[card_order.index(last_card)+<span class=hljs-number >1</span>:]

i = <span class=hljs-number >0</span>
<span class=hljs-keyword >while</span> i &lt; num_el:
    <span class=hljs-keyword >if</span> r1[i] == r2[i] <span class=hljs-keyword >or</span> r1[i]==<span class=hljs-string >&quot;  &quot;</span> <span class=hljs-keyword >or</span> r2[i]==<span class=hljs-string >&quot;  &quot;</span>:
        i += <span class=hljs-number >1</span>
        <span class=hljs-keyword >continue</span>
    <span class=hljs-keyword >elif</span> i &gt; <span class=hljs-built_in >len</span>(r1):
        r1.append(<span class=hljs-string >&quot;  &quot;</span>)
    <span class=hljs-keyword >elif</span> i &gt; <span class=hljs-built_in >len</span>(r2):
        r2.append(<span class=hljs-string >&quot;  &quot;</span>)
    <span class=hljs-keyword >elif</span> r1[i] == r2[i+<span class=hljs-number >1</span>]:
        r1.insert(i,<span class=hljs-string >&quot;  &quot;</span>)
    <span class=hljs-keyword >elif</span> r2[i] == r1[i+<span class=hljs-number >1</span>]:
        r2.insert(i,<span class=hljs-string >&quot;  &quot;</span>)
    <span class=hljs-keyword >elif</span> r1[i] == r2[i+<span class=hljs-number >2</span>]:
        r1.insert(i,<span class=hljs-string >&quot;  &quot;</span>)
    <span class=hljs-keyword >elif</span> r2[i] == r1[i+<span class=hljs-number >2</span>]:
        r2.insert(i,<span class=hljs-string >&quot;  &quot;</span>)
    i = <span class=hljs-number >0</span>

[<span class=hljs-built_in >print</span>(i,j) <span class=hljs-keyword >for</span> i,j <span class=hljs-keyword >in</span> <span class=hljs-built_in >zip</span>(r1,r2)]</code></pre>
<pre><code class="Plaintext hljs">After alignment first pass:

   9d
   as
0d 0d
8c 8c
jc jc
0h 0h
jh jh
   8s
9s 9s
9d 9d
kh kh
0s 0s
9h 9h</code></pre>
<p>Then we can crawl through again and determine what to do with the empty characters. We either toss them or infill based on how many other entries for that particular card we see elsewhere:</p>
<pre><code class="Python hljs"><span class=hljs-built_in >ord</span> = []
<span class=hljs-keyword >for</span> (e1,e2) <span class=hljs-keyword >in</span> <span class=hljs-built_in >zip</span>(r1, r2):
    <span class=hljs-keyword >if</span> e1 == <span class=hljs-string >&quot;  &quot;</span>:
        <span class=hljs-keyword >if</span> card_order.count(e2) &gt; <span class=hljs-number >1</span>:
            <span class=hljs-keyword >continue</span>
        <span class=hljs-keyword >else</span>:
            <span class=hljs-built_in >ord</span>.append(e2)
    <span class=hljs-keyword >elif</span> e2 == <span class=hljs-string >&quot;  &quot;</span>:
        <span class=hljs-keyword >if</span> card_order.count(e1) &gt; <span class=hljs-number >1</span>:
            <span class=hljs-keyword >continue</span>
        <span class=hljs-keyword >else</span>:
            <span class=hljs-built_in >ord</span>.append(e1)
    <span class=hljs-keyword >elif</span> e1 == e2:
        <span class=hljs-built_in >ord</span>.append(e1)</code></pre>
<pre><code class="Plaintext hljs">as
0d
8c
jc
0h
jh
8s
9s
9d
kh
0s
9h</code></pre>
<p>Almost there. Our last step is to link two deck orders and back out the shuffle sequence. For example, we might see these two deck orders in a row:</p>
<pre><code class="Plaintext hljs">as  8s
0d  as
8c  0d
jc  9s
0h  8c
jh  9d
8s  kh
9s  jc
9d  0h
kh  jh
0s  0s
9h  9h</code></pre>
<p>To back out the order we first find where the cut happened and then iterate through the resulting deck to see which hand each card came out of:</p>
<pre><code class="Python hljs"><span class=hljs-keyword >def</span> <span class="hljs-title function_">get_shuffle</span>(<span class=hljs-params >o1, o2</span>):
    i = <span class=hljs-number >0</span>
    <span class=hljs-keyword >for</span> e <span class=hljs-keyword >in</span> o2:
        <span class=hljs-keyword >if</span> e==o1[i]:
            i+=<span class=hljs-number >1</span>
        <span class=hljs-keyword >else</span>:
            cut_loc = i
    lh = o1[:cut_loc]
    rh = o1[cut_loc:]
    
    o = <span class=hljs-string >&quot;&quot;</span>
    il = <span class=hljs-number >0</span>
    ir = <span class=hljs-number >0</span>
    <span class=hljs-keyword >for</span> e <span class=hljs-keyword >in</span> o2:
        <span class=hljs-keyword >if</span> il&lt;<span class=hljs-built_in >len</span>(lh) <span class=hljs-keyword >and</span> e==lh[il]:
            o += <span class=hljs-string >&quot;1&quot;</span>
            il += <span class=hljs-number >1</span>
        <span class=hljs-keyword >elif</span> e==rh[ir]:
            o += <span class=hljs-string >&quot;2&quot;</span>
            ir += <span class=hljs-number >1</span>
        <span class=hljs-keyword >else</span>:
            ValueError(<span class=hljs-string >&quot;not feasible shuffle result&quot;</span>)
            
    <span class=hljs-keyword >return</span> o</code></pre>
<pre><code class="Python hljs">o1 = [<span class=hljs-string >&quot;as&quot;</span>,<span class=hljs-string >&quot;0d&quot;</span>,<span class=hljs-string >&quot;8c&quot;</span>,<span class=hljs-string >&quot;jc&quot;</span>,<span class=hljs-string >&quot;0h&quot;</span>,<span class=hljs-string >&quot;jh&quot;</span>,<span class=hljs-string >&quot;8s&quot;</span>,<span class=hljs-string >&quot;9s&quot;</span>,<span class=hljs-string >&quot;9d&quot;</span>,<span class=hljs-string >&quot;kh&quot;</span>,<span class=hljs-string >&quot;0s&quot;</span>,<span class=hljs-string >&quot;9h&quot;</span>]
o2 = [<span class=hljs-string >&quot;8s&quot;</span>,<span class=hljs-string >&quot;as&quot;</span>,<span class=hljs-string >&quot;0d&quot;</span>,<span class=hljs-string >&quot;9s&quot;</span>,<span class=hljs-string >&quot;8c&quot;</span>,<span class=hljs-string >&quot;9d&quot;</span>,<span class=hljs-string >&quot;kh&quot;</span>,<span class=hljs-string >&quot;jc&quot;</span>,<span class=hljs-string >&quot;0h&quot;</span>,<span class=hljs-string >&quot;jh&quot;</span>,<span class=hljs-string >&quot;0s&quot;</span>,<span class=hljs-string >&quot;9h&quot;</span>]
get_shuffle(o1,o2)</code></pre>
<pre><code class="Plaintext hljs">&#x27;211212211122&#x27;</code></pre>
<p>Whew. A lengthy process but the result is a decent data collection pipeline.</p>
<h1 id=building_a_shuffling_model ><a href="#building_a_shuffling_model" class=header-anchor >Building a Shuffling Model</a></h1>
<p>A recording of 100 of my shuffles can be found <a href="https://raw.githubusercontent.com/jacobwood27/031_shuffle/main/rec.txt">here</a>.</p>
<p>This part of the project is all done in <a href="https://julialang.org/">Julia</a>.</p>
<h2 id=data_exploration ><a href="#data_exploration" class=header-anchor >Data Exploration</a></h2>
<p>Bring in a few packages</p>
<pre><code class="julia hljs"><span class=hljs-keyword >using</span> Plots, StatsPlots
<span class=hljs-keyword >using</span> Random
<span class=hljs-keyword >using</span> Distributions
<span class=hljs-keyword >using</span> StatsBase
<span class=hljs-keyword >using</span> KernelDensity
<span class=hljs-keyword >using</span> Trapz</code></pre>
<p>And read in the data from the recorded file into a vector of vectors of <code>Int</code>.</p>
<pre><code class="julia hljs">S_rec = [[parse(<span class=hljs-built_in >Int</span>,c) <span class=hljs-keyword >for</span> c <span class=hljs-keyword >in</span> l] <span class=hljs-keyword >for</span> l <span class=hljs-keyword >in</span> readlines(<span class=hljs-string >&quot;rec.txt&quot;</span>)]</code></pre>
<p>One way we can &quot;score&quot; a shuffle is by counting the number of card runs there were. A perfect shuffle, with two piles of 26 cards interwoven one at a time, would score 52 on this metric.</p>
<pre><code class="julia hljs"><span class=hljs-keyword >function</span> score_shuffle(S)
    score = <span class=hljs-number >1</span>
    <span class=hljs-keyword >for</span> i <span class=hljs-keyword >in</span> <span class=hljs-number >2</span>:<span class=hljs-number >52</span>
        <span class=hljs-keyword >if</span> S[i] != S[i-<span class=hljs-number >1</span>]
            score += <span class=hljs-number >1</span>
        <span class=hljs-keyword >end</span>
    <span class=hljs-keyword >end</span>
    score
<span class=hljs-keyword >end</span></code></pre>
<pre><code class="julia hljs">scores = score_shuffle.(S_rec)
plot(scores, st=:scatter, legend=<span class=hljs-literal >false</span>, smooth=<span class=hljs-literal >true</span>,
    xlabel=<span class=hljs-string >&quot;Row&quot;</span>, ylabel=<span class=hljs-string >&quot;Score&quot;</span>)</code></pre>
<div class=im-100 ><img src="/posts/005_shuffle/shuffle_scores_over_time.svg" alt=""></div>
<p>Ooph. A large range of performance and definitely got tired over time. But is this any good? It is tough to evaluate without context. The <a href="https://en.wikipedia.org/wiki/Gilbert&#37;E2&#37;80&#37;93Shannon&#37;E2&#37;80&#37;93Reeds_model">Gilbert-Shannon-Reeds &#40;GSR&#41; model</a> dates back to 1955 and is the de-facto distribution used to model a riffle shuffle. The model has two steps: </p>
<ol>
<li><p>Split the deck into left and right piles &#40;<span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal">A</span></span></span></span> and <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span></span></span></span>&#41; according to a binomial distribution</p>

<li><p>Drop the cards one at a time with probability <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy=false >(</mo><mi>A</mi><mo stretchy=false >)</mo><mo>=</mo><mi>A</mi><mi mathvariant=normal >/</mi><mo stretchy=false >(</mo><mi>A</mi><mo>+</mo><mi>B</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">P(A) = A/(A+B)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class=mopen >(</span><span class="mord mathnormal">A</span><span class=mclose >)</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">A</span><span class=mord >/</span><span class=mopen >(</span><span class="mord mathnormal">A</span><span class=mspace  style="margin-right:0.2222222222222222em;"></span><span class=mbin >+</span><span class=mspace  style="margin-right:0.2222222222222222em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class=mclose >)</span></span></span></span></p>

</ol>
<p>This can be implemented in Julia:</p>
<pre><code class="julia hljs"><span class=hljs-keyword >function</span> gsr_shuffle()
    cut = rand(Binomial(<span class=hljs-number >52</span>),<span class=hljs-number >1</span>)[<span class=hljs-number >1</span>]
    nl = cut
    nr = <span class=hljs-number >52</span> - cut

    out = zeros(<span class=hljs-built_in >Int</span>,<span class=hljs-number >52</span>)
    <span class=hljs-keyword >for</span> i <span class=hljs-keyword >in</span> <span class=hljs-number >1</span>:<span class=hljs-number >52</span>
        pl = nl / (nl + nr)
        <span class=hljs-keyword >if</span> rand() &lt; pl
            out[i] = <span class=hljs-number >1</span>
            nl -= <span class=hljs-number >1</span>
        <span class=hljs-keyword >else</span>
            out[i] = <span class=hljs-number >2</span>
            nr -= <span class=hljs-number >1</span>
        <span class=hljs-keyword >end</span>
    <span class=hljs-keyword >end</span>

    out
<span class=hljs-keyword >end</span></code></pre>
<p>Now we can make a bunch of GSR-shuffled decks and see how our scores compare.</p>
<pre><code class="julia hljs">S_scores = score_shuffle.(S_vec)

GSR_vec = [gsr_shuffle() <span class=hljs-keyword >for</span> _ <span class=hljs-keyword >in</span> <span class=hljs-number >1</span>:<span class=hljs-number >10000</span>]
GSR_scores = score_shuffle.(GSR_vec)

histogram(scores,      label=<span class=hljs-string >&quot;Me&quot;</span>,  bins=<span class=hljs-number >15</span>:<span class=hljs-number >2</span>:<span class=hljs-number >52</span>, lw=<span class=hljs-number >2</span>, alpha=<span class=hljs-number >0.5</span>, norm=:probability)
histogram!(GSR_scores, label=<span class=hljs-string >&quot;GSR&quot;</span>, bins=<span class=hljs-number >15</span>:<span class=hljs-number >2</span>:<span class=hljs-number >52</span>, lw=<span class=hljs-number >2</span>, alpha=<span class=hljs-number >0.5</span>, norm=:probability,
    xlabel=<span class=hljs-string >&quot;Score&quot;</span>, ylabel=<span class=hljs-string >&quot;Fraction&quot;</span>)</code></pre>
<div class=im-100 ><img src="/posts/005_shuffle/score_comp_hist.svg" alt=""></div>
<p>Looks like we outperform the shuffling model by quite a bit on this metric&#33; Our mean score turns out to be 33 vs 27 for the GSR shuffle. This indicates we should probably build our own model instead of assuming the GSR.</p>
<h2 id=building_a_model ><a href="#building_a_model" class=header-anchor >Building a Model</a></h2>
<h3 id=deck_split ><a href="#deck_split" class=header-anchor >Deck Split</a></h3>
<p>The first action in shuffling is splitting the deck into two halves. Ideally you end up with two stacks of 26 cards. The standard approach to modeling this action &#40;the approach taken in the GSR model&#41; is to draw the card split from a binomial distribution. Let&#39;s see if that looks decent for us.</p>
<pre><code class="julia hljs">split_vec = [count(s.==<span class=hljs-number >1</span>) <span class=hljs-keyword >for</span> s <span class=hljs-keyword >in</span> S_vec]
histogram(split_vec, bins=<span class=hljs-number >16.5</span>:<span class=hljs-number >1</span>:<span class=hljs-number >35.5</span>, xticks=<span class=hljs-number >17</span>:<span class=hljs-number >35</span>, normalize=:pdf, label=<span class=hljs-string >&quot;observed&quot;</span>)
plot!(Binomial(<span class=hljs-number >52</span>), st=:line, xlims=(<span class=hljs-number >17</span>,<span class=hljs-number >35</span>), label = <span class=hljs-string >&quot;expected (binomial)&quot;</span>,
    xlabel=<span class=hljs-string >&quot;Number of Cards in Left Hand&quot;</span>, ylabel=<span class=hljs-string >&quot;Fraction of Cases&quot;</span>)</code></pre>
<div class=im-100 ><img src="/posts/005_shuffle/split_hist.svg" alt=""></div>
<p>The binomial distribution is not a great fit here. 84/100 cases I ended up with &lt;26 cards in my left hand and the distribution is much sharper &#40;centered around 24&#41; than the binomial would indicate. We should fit a different distribution to this action. </p>
<p>Our distribution is discrete and univariate over the integers from &#40;let&#39;s say&#41; 16 to 36. We can come up with a few ways of generating the underlying distribution:</p>
<ul>
<li><p>Use the measured histogram</p>

<li><p>Fit a known discrete distribution to the data</p>

<li><p>Fit a known continuous distribution to the data and round</p>

<li><p>Use the data to generate a kernel density estimator</p>

</ul>
<h4 id=measured_histogram ><a href="#measured_histogram" class=header-anchor >Measured Histogram</a></h4>
<p>The most straightforward way to translate our measurements into a discrete probability distribution is to assume the data directly describes the underlying distribution. This assumption is most likely to hold true when you have a lot of measured data that has borne out the entirety of the underlying distribution. This assumption may appear to be true when the measured histogram is smooth and well-shaped. </p>
<p>The data we have shows a 1&#37; chance of drawing a 30, and a 0&#37; chance of drawing a 29. This is a small discrepancy but seems unlikely to me to be true.</p>
<p>The resultant PMF would be identical to the histogram of measurements:</p>
<div class=im-100 ><img src="/posts/005_shuffle/hist_model.svg" alt=""></div>
<h4 id=known_discrete_distribution ><a href="#known_discrete_distribution" class=header-anchor >Known Discrete Distribution</a></h4>
<p>The <a href="https://juliastats.org/Distributions.jl/stable/">Distributions.jl</a> package provides a collection of distributions and the ability to fit them to experimental data using &#40;usually&#41; maximum likelihood estimation. The distributions available for this functionality are shown below. </p>
<ul>
<li><p><a href="https://en.wikipedia.org/wiki/Binomial_distribution">Binomial&#40;52&#41;</a> - would result from drawing each card as either right or left with a fixed probability</p>

<li><p><a href="https://en.wikipedia.org/wiki/Binomial_distribution">Binomial&#40;20&#41;</a> - would result from only randomly drawing the 20 middle cards with a fixed probability &#40;and assuming the other 32 are split 16 left, 16 right&#41;</p>

<li><p><a href="https://en.wikipedia.org/wiki/Discrete_uniform_distribution">Discrete Uniform</a> - would result if every outcome in the possible solution space has the same probability</p>

<li><p><a href="https://en.wikipedia.org/wiki/Geometric_distribution">Geometric</a> - usually interpreted as the number of trials needed for a single outcome to materialize. Tough to apply in this case.</p>

<li><p><a href="https://en.wikipedia.org/wiki/Poisson_distribution">Poisson</a> - the probability of a given number of <em>independent</em> events occurring in a specific period of time. We don&#39;t expect card dropping events to be independent. </p>

</ul>
<p>Among these distributions we expect the binomial &#40;especially the binomial with a smaller number of chance draws&#41; to perform the best. And indeed that is true, the Binomial&#40;20&#41; distribution &#40;which resulted in a 39&#37; chance that each of the 20 randomly drawn card ends up in my left hand&#41; fits the data decently well as seen in the chart below. </p>
<p>One thing we might &#40;definitely&#41; want to do is truncate the distribution - we certainly don&#39;t ever want our split to be &lt;0 or &gt;52, and we probably wouldn&#39;t shuffle the cards if we had &lt;16 or &gt;36 cards in one hand, it just doesn&#39;t feel right. We&#39;ll apply 16-36 truncation in all the results going forward.</p>
<pre><code class="julia hljs">split_vec = [count(s.==<span class=hljs-number >1</span>) <span class=hljs-keyword >for</span> s <span class=hljs-keyword >in</span> S_vec]
histogram(split_vec, bins=<span class=hljs-number >15.5</span>:<span class=hljs-number >36.5</span>, norm=:pdf, label=<span class=hljs-string >&quot;Measured&quot;</span>, alpha=<span class=hljs-number >0.5</span>)

f = fit_mle(Binomial, <span class=hljs-number >52</span>, split_vec)
f = truncated(f,<span class=hljs-number >16</span>,<span class=hljs-number >36</span>)
plot!(f, st=:line, label = <span class=hljs-string >&quot;Binomial(52)&quot;</span>, lw=<span class=hljs-number >2</span>)

x = <span class=hljs-number >16</span>:<span class=hljs-number >36</span>
f = fit_mle(Binomial, x[<span class=hljs-keyword >end</span>]-x[<span class=hljs-number >1</span>], split_vec.-x[<span class=hljs-number >1</span>])
plot!(x, st=:line, pdf.(f, x.-x[<span class=hljs-number >1</span>]), label = <span class=hljs-string >&quot;Binomial(20)&quot;</span>, lw=<span class=hljs-number >2</span>)

distributions = [   
    (DiscreteUniform,   <span class=hljs-string >&quot;DiscreteUniform&quot;</span>)
    (Geometric,         <span class=hljs-string >&quot;Geometric&quot;</span>)
    (Poisson,           <span class=hljs-string >&quot;Poisson&quot;</span>)
]
<span class=hljs-keyword >for</span> d <span class=hljs-keyword >in</span> distributions
    f = fit_mle(d[<span class=hljs-number >1</span>], split_vec)
    f = truncated(f,<span class=hljs-number >16</span>,<span class=hljs-number >36</span>)
    plot!(f, st=:line, marker=<span class=hljs-literal >false</span>, label = d[<span class=hljs-number >2</span>], lw=<span class=hljs-number >2</span>)
<span class=hljs-keyword >end</span>
plot!(xlabel=<span class=hljs-string >&quot;Number of Cards in Left Hand&quot;</span>, ylabel=<span class=hljs-string >&quot;Fraction of Cases&quot;</span>, xlims=(<span class=hljs-number >16</span>,<span class=hljs-number >36</span>), xticks=<span class=hljs-number >16</span>:<span class=hljs-number >36</span>)</code></pre>
<p>Note - discrete distributions are plotted here as continuous for ease of viewing. </p>
<div class=im-100 ><img src="/posts/005_shuffle/discrete_fits.svg" alt=""></div>
<h4 id=known_continuous_distribution ><a href="#known_continuous_distribution" class=header-anchor >Known Continuous Distribution</a></h4>
<p>Similarly to the discrete options, Distributions.jl provides a host of continuous distributions that can be easily fit to our experimental data. </p>
<pre><code class="julia hljs">distributions = [   
    (Exponential,       <span class=hljs-string >&quot;Exponential&quot;</span>)
    (LogNormal,         <span class=hljs-string >&quot;LogNormal&quot;</span>)
    (Normal,            <span class=hljs-string >&quot;Normal&quot;</span>)
    (Gamma,             <span class=hljs-string >&quot;Gamma&quot;</span>)
    (Laplace,           <span class=hljs-string >&quot;Laplace&quot;</span>)
    (Pareto,            <span class=hljs-string >&quot;Pareto&quot;</span>)
    (Poisson,           <span class=hljs-string >&quot;Poisson&quot;</span>)
    (Rayleigh,          <span class=hljs-string >&quot;Rayleigh&quot;</span>)
    (InverseGaussian,   <span class=hljs-string >&quot;InverseGaussian&quot;</span>)
    (Uniform,           <span class=hljs-string >&quot;Uniform&quot;</span>)
    (Weibull,           <span class=hljs-string >&quot;Weibull&quot;</span>)
]

histogram(split_vec, bins=<span class=hljs-number >15.5</span>:<span class=hljs-number >36.5</span>, norm=:pdf, label=<span class=hljs-string >&quot;observed&quot;</span>, alpha=<span class=hljs-number >0.5</span>)

<span class=hljs-keyword >for</span> d <span class=hljs-keyword >in</span> distributions
    f = fit_mle(d[<span class=hljs-number >1</span>], split_vec)
    f = truncated(f,<span class=hljs-number >16</span>,<span class=hljs-number >36</span>)
    plot!(f, st=:line, label = d[<span class=hljs-number >2</span>], lw=<span class=hljs-number >2</span>)
<span class=hljs-keyword >end</span>
plot!(xlabel=<span class=hljs-string >&quot;Number of Cards in Left Hand&quot;</span>, ylabel=<span class=hljs-string >&quot;Fraction of Cases&quot;</span>, xlims=(<span class=hljs-number >16</span>,<span class=hljs-number >36</span>), xticks=<span class=hljs-number >16</span>:<span class=hljs-number >36</span>)</code></pre>
<div class=im-100 ><img src="/posts/005_shuffle/all_continuous.svg" alt=""></div>
<p>We can discount quite a few of these right off the bat and then we are left with the LogNormal, Normal, Gamma, and InverseGaussian distributions.</p>
<ul>
<li><p><a href="https://en.wikipedia.org/wiki/Log-normal_distribution">Log Normal</a> - usually the result of an event which is the product of multiple independent random variables</p>

<li><p><a href="https://en.wikipedia.org/wiki/Normal_distribution">Normal</a> - usually the result of an event which is the sum of multiple independent random variables</p>

<li><p><a href="https://en.wikipedia.org/wiki/Gamma_distribution">Gamma</a> - can be used to model wait times - such as when will the nth event occur?</p>

<li><p><a href="https://en.wikipedia.org/wiki/Inverse_Gaussian_distribution">Inverse Gaussian</a> - if a normal distribution describes possible values of a random walk process at a fixed time, the inverse gaussian describes the possible times at which we might see a specific value of a random walk process. </p>

</ul>
<p>In order to round these to the proper domain we can integrate them over the rounding range of each integer. In the end these all look practically identical:</p>
<pre><code class="julia hljs">distributions = [   
    (LogNormal,         <span class=hljs-string >&quot;LogNormal&quot;</span>)
    (Normal,            <span class=hljs-string >&quot;Normal&quot;</span>)
    (Gamma,             <span class=hljs-string >&quot;Gamma&quot;</span>)
    (InverseGaussian,   <span class=hljs-string >&quot;InverseGaussian&quot;</span>)
]

histogram(split_vec, bins=<span class=hljs-number >15.5</span>:<span class=hljs-number >36.5</span>, norm=:pdf, label=<span class=hljs-string >&quot;observed&quot;</span>, alpha=<span class=hljs-number >0.5</span>)


<span class=hljs-keyword >for</span> d <span class=hljs-keyword >in</span> distributions
    f = fit_mle(d[<span class=hljs-number >1</span>], split_vec)
    f = truncated(f,<span class=hljs-number >16</span>,<span class=hljs-number >36</span>)

    <span class=hljs-comment >#Integrate</span>
    x_ep = <span class=hljs-number >15.5</span>:<span class=hljs-number >36.5</span>
    x_cp = <span class=hljs-number >16</span>:<span class=hljs-number >36</span>
    y_cp = [cdf(f, x_ep[i+<span class=hljs-number >1</span>]) - cdf(f, x_ep[i]) <span class=hljs-keyword >for</span> i <span class=hljs-keyword >in</span> <span class=hljs-number >1</span>:length(x_ep)-<span class=hljs-number >1</span>]

    plot!(x_cp, y_cp, st=:line, label = d[<span class=hljs-number >2</span>], lw=<span class=hljs-number >2</span>)
<span class=hljs-keyword >end</span>
plot!(xlabel=<span class=hljs-string >&quot;Number of Cards in Left Hand&quot;</span>, ylabel=<span class=hljs-string >&quot;Fraction of Cases&quot;</span>, xlims=(<span class=hljs-number >16</span>,<span class=hljs-number >36</span>), xticks=<span class=hljs-number >16</span>:<span class=hljs-number >36</span>)</code></pre>
<div class=im-100 ><img src="/posts/005_shuffle/few_continuous.svg" alt=""></div>
<p>You might be able to make a case for any of these. They all can fit the data we have pretty well. If pressed I might argue the normal distribution makes the most sense here so that&#39;s what I&#39;ll assume going forward.</p>
<h4 id=kernel_density_estimate ><a href="#kernel_density_estimate" class=header-anchor >Kernel Density Estimate</a></h4>
<p>The other option we have when generating a distribution here is to take the measured histogram and apply some smoothing via <a href="https://en.wikipedia.org/wiki/Kernel_density_estimation">kernel density estimation</a>. This is a great choice if we ever just want to make sure we are drawing from something very close to the measured data. That might be nice if the underlying mechanisms are poorly understood or too complex to attempt to model.</p>
<p>The Julia package <a href="https://github.com/JuliaStats/KernelDensity.jl">KernelDensity.jl</a> makes this process easy. The main parameter of interest is the kernel bandwidth we use to smooth with - larger values will smooth over spurious measurements at the expense of pulling down peaks. Smaller measurements won&#39;t do much smoothing work and may capture more fine structure than you would hope. </p>
<p>This is also a continuous estimator - so we need to integrate over the relevant region to get a probability mass function over the integers.</p>
<pre><code class="julia hljs">histogram(split_vec, bins=<span class=hljs-number >15.5</span>:<span class=hljs-number >36.5</span>, norm=:pdf, label=<span class=hljs-string >&quot;observed&quot;</span>, alpha=<span class=hljs-number >0.5</span>)

<span class=hljs-keyword >for</span> bw = [<span class=hljs-number >0.2</span>, <span class=hljs-number >0.5</span>, <span class=hljs-number >1.0</span>, <span class=hljs-number >2.0</span>]
    k = kde(split_vec, boundary=(<span class=hljs-number >26</span>-<span class=hljs-number >10</span>, <span class=hljs-number >26</span>+<span class=hljs-number >10</span>), bandwidth=bw)
    
    x = <span class=hljs-number >16</span>:<span class=hljs-number >0.01</span>:<span class=hljs-number >36</span>
    y = pdf(k, x)

    x_ep = <span class=hljs-number >15.5</span>:<span class=hljs-number >36.5</span>
    x_cp = <span class=hljs-number >16</span>:<span class=hljs-number >36</span>
    y_cp = [trapz(x[(x.&gt;x_ep[i]) .&amp; (x.&lt;x_ep[i+<span class=hljs-number >1</span>])], y[(x.&gt;x_ep[i]) .&amp; (x.&lt;x_ep[i+<span class=hljs-number >1</span>])]) <span class=hljs-keyword >for</span> i <span class=hljs-keyword >in</span> <span class=hljs-number >1</span>:length(x_ep)-<span class=hljs-number >1</span>]
    
    plot!(x_cp, y_cp, lw=<span class=hljs-number >2</span>, label=<span class=hljs-string >&quot;BW = &quot;</span> * string(bw))
<span class=hljs-keyword >end</span>

plot!(xlabel=<span class=hljs-string >&quot;Number of Cards in Left Hand&quot;</span>, ylabel=<span class=hljs-string >&quot;Fraction of Cases&quot;</span>, xlims=(<span class=hljs-number >16</span>,<span class=hljs-number >36</span>), xticks=<span class=hljs-number >16</span>:<span class=hljs-number >36</span>)</code></pre>
<div class=im-100 ><img src="/posts/005_shuffle/kernel_fit.svg" alt=""></div>
<p>A bandwidth of 0.5 smooths over the spurious 29-30 bump while retaining most of the rest of the structure, it probably wins the eye test here. </p>
<h4 id=winning_distribution ><a href="#winning_distribution" class=header-anchor >Winning Distribution</a></h4>
<p>We can compare the leading solutions from each of the previous 4 sections:</p>
<div class=im-100 ><img src="/posts/005_shuffle/4best.svg" alt=""></div>
<p>At this point it comes down to either our understanding of the underlying mechanism that is driving the distribution or aesthetic preference. Let me know if you come up with a good causal model. In the meantime, I&#39;ll use the normal distribution with a mean of 23.8 and a standard deviation of 1.8.</p>
<h3 id=dropping_cards ><a href="#dropping_cards" class=header-anchor >Dropping Cards</a></h3>
<p>After splitting the deck in two we start to drop cards from either our left or right hand. The GSJ model predicts that this will happen probabilistically, with the probability of dropping from either the left or the right according solely to the fraction of remaining total cards that are currently in that hand. Is this a good model for us?</p>
<h4 id=gsr_calibration ><a href="#gsr_calibration" class=header-anchor >GSR Calibration</a></h4>
<p>We can view GSR as making a prediction each time we are about to drop a card. We can then look at what actually happened, and see if things that were supposed to happen 25&#37; of the time actually happened 25&#37; of the time. This kind of analysis is commonly referred to as <a href="https://en.wikipedia.org/wiki/Calibration_&#40;statistics&#41;">model calibration</a>. </p>
<p>We don&#39;t have perfect point estimates here - so we need to bucket the probabalities. Anything within the blue squares below is considered likely &quot;calibrated&quot;. For example, the bottom left point says that whenever the model predicted an event to happen between 0-10&#37; of the time, it actually happened 0&#37; of the time. </p>
<p>The points are sized roughly corresponding to how many predictions they represent. We notice that the model does a pretty good job overall - the only time it is off by more than 10&#37; is the 10-20&#37; bucket which is only comprised of 50 observations. Not bad.</p>
<pre><code class="julia hljs">probs = <span class=hljs-built_in >Float64</span>[]
outcs = <span class=hljs-built_in >Bool</span>[]
cards = <span class=hljs-built_in >Int</span>[]
<span class=hljs-keyword >for</span> S <span class=hljs-keyword >in</span> S_vec
    <span class=hljs-keyword >for</span> (i,s) <span class=hljs-keyword >in</span> enumerate(S)
        nl = count(S[i:<span class=hljs-keyword >end</span>].==<span class=hljs-number >1</span>)
        nr = count(S[i:<span class=hljs-keyword >end</span>].==<span class=hljs-number >2</span>)
        prob = nl/(nl+nr)
        outc = (s == <span class=hljs-number >1</span>)
        push!(probs, prob)
        push!(outcs, outc)
        push!(cards, nl+nr)
    <span class=hljs-keyword >end</span>
<span class=hljs-keyword >end</span>

edges = <span class=hljs-number >0</span>:<span class=hljs-number >0.1</span>:<span class=hljs-number >1.0</span>
binC  = <span class=hljs-number >0.05</span>:<span class=hljs-number >0.1</span>:<span class=hljs-number >0.95</span>

h  = fit(Histogram, probs, edges)
binindices = StatsBase.binindex.(<span class=hljs-built_in >Ref</span>(h), probs)

outF = <span class=hljs-built_in >Float64</span>[]
dps  = <span class=hljs-built_in >Int</span>[]
<span class=hljs-keyword >for</span> i <span class=hljs-keyword >in</span> <span class=hljs-number >1</span>:length(binC)
    idx = binindices .== i
    push!(outF, count(outcs[idx]) / count(idx))
    push!(dps, count(idx))
<span class=hljs-keyword >end</span>

X = vcat([[x-<span class=hljs-number >0.05</span>,x-<span class=hljs-number >0.05</span>,x+<span class=hljs-number >0.05</span>,x+<span class=hljs-number >0.05</span>,x-<span class=hljs-number >0.05</span>,<span class=hljs-literal >NaN</span>] <span class=hljs-keyword >for</span> x <span class=hljs-keyword >in</span> binC]...)
Y = vcat([[x-<span class=hljs-number >0.05</span>,x+<span class=hljs-number >0.05</span>,x+<span class=hljs-number >0.05</span>,x-<span class=hljs-number >0.05</span>,x-<span class=hljs-number >0.05</span>,<span class=hljs-literal >NaN</span>] <span class=hljs-keyword >for</span> x <span class=hljs-keyword >in</span> binC]...)
plot(X, Y, fill=<span class=hljs-literal >true</span>, label=<span class=hljs-string >&quot;Perfectly Calibrated&quot;</span>, aspect_ratio=<span class=hljs-number >1</span>, size=(<span class=hljs-number >500</span>,<span class=hljs-number >500</span>),
    xlabel = <span class=hljs-string >&quot;Predicted Probability&quot;</span>,
    ylabel = <span class=hljs-string >&quot;Observed Probability&quot;</span>,
    xticks = edges, yticks=edges)

outF[dps.&lt;<span class=hljs-number >10</span>] .= <span class=hljs-number >0</span>
dps[dps.&lt;<span class=hljs-number >10</span>]  .= <span class=hljs-number >0</span>
binX = [mean(probs[binindices.==i]) <span class=hljs-keyword >for</span> i <span class=hljs-keyword >in</span> <span class=hljs-number >1</span>:length(dps)]
scatter!(binX, outF, markersize=log.(dps/maximum(dps)).+<span class=hljs-number >9</span>, label=<span class=hljs-string >&quot;Actual&quot;</span>, legend=:topleft, xlims=(-<span class=hljs-number >0.05</span>,<span class=hljs-number >1.05</span>), ylims=(-<span class=hljs-number >0.05</span>,<span class=hljs-number >1.05</span>))</code></pre>
<div class=im-100 ><img src="/posts/005_shuffle/my_calibration.svg" alt=""></div>
<h4 id=differences_from_gsr ><a href="#differences_from_gsr" class=header-anchor >Differences from GSR</a></h4>
<p>What are some ways in which we might differ from the GSR model? Perhaps we systematically favor the right or left hand at certain times:</p>
<pre><code class="julia hljs">side_ev = [mean([s[i] <span class=hljs-keyword >for</span> s <span class=hljs-keyword >in</span> S_vec]) <span class=hljs-keyword >for</span> i <span class=hljs-keyword >in</span> <span class=hljs-number >1</span>:<span class=hljs-number >52</span>] .- <span class=hljs-number >1</span>
plot(side_ev, label=<span class=hljs-string >&quot;observed&quot;</span>, ylims=(<span class=hljs-number >0</span>, <span class=hljs-number >1</span>), ylabel=<span class=hljs-string >&quot; Left Hand                Right Hand &quot;</span>, xlabel=<span class=hljs-string >&quot;Card Number&quot;</span>, lw=<span class=hljs-number >2</span>)

GSR_side_ev = [mean([s[i] <span class=hljs-keyword >for</span> s <span class=hljs-keyword >in</span> GSR_vec]) <span class=hljs-keyword >for</span> i <span class=hljs-keyword >in</span> <span class=hljs-number >1</span>:<span class=hljs-number >52</span>] .- <span class=hljs-number >1</span>
plot!(GSR_side_ev, label=<span class=hljs-string >&quot;GSR model&quot;</span>, lw=<span class=hljs-number >2</span>)</code></pre>
<div class=im-100 ><img src="/posts/005_shuffle/side_expected.svg" alt=""></div>
<p>It looks like we do systematically favor the right hand at the beginning and end of the shuffle.</p>
<p>Of particular interest in shuffling efficiency is runs of cards, do we have more or longer runs than the GSR model would predict?</p>
<h1 id=investigating_performance ><a href="#investigating_performance" class=header-anchor >Investigating Performance</a></h1>
<p>&#91;IN WORK&#93;</p>
<h1 id=ingredients ><a href="#ingredients" class=header-anchor >Ingredients</a></h1>
<ul>
<li><p>pngquant</p>

<li><p>Gimp</p>

<li><p>ffmpeg</p>

<li><p>jpegoptim</p>

<li><p>iPhone 11</p>

<li><p>cardboard</p>

<li><p>Bicycle playing cards</p>

<li><p>Python 3.9.7</p>

<li><p>OpenCV 4.5.4</p>

<li><p>Matplotlib 3.3.4</p>

<li><p>Numpy 1.19.5</p>

<li><p>Tensorflow 2.7.0</p>

<li><p>Tensorflow_hub 0.12.0</p>

<li><p>Julia 1.7.1</p>

<li><p><a href="https://juliastats.org/Distributions.jl/stable/">Distributions.jl</a></p>

</ul>
<div class=page-foot >
  <div class=copyright >
    <a href="mailto:mail@jacobw.xyz" class="fa fa-envelope"></a> <a href="https://github.com/jacobwood27" class="fa fa-github"></a>
     <br>
     <br>
    Last modified: April 18, 2025. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>.
  </div>
</div>
</div>